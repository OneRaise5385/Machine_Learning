{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量化金融预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import sklearn.tree as tree\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss, mean_squared_log_error\n",
    "import sys, os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据导入\n",
    "train_file = os.listdir('input/train/')\n",
    "test_file = os.listdir('input/test/')\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "# 读取训练数据\n",
    "for file in train_file:\n",
    "    tmp = pd.read_csv('input/train/'+file)\n",
    "    tmp['file'] = file\n",
    "    train = pd.concat([train, tmp], axis=0, ignore_index=True)\n",
    "# 读取测试数据\n",
    "for file in test_file:\n",
    "    tmp = pd.read_csv('input/test/'+file)\n",
    "    tmp['file'] = file\n",
    "    test = pd.concat([test, tmp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['n_bid1','n_bid2','n_ask1','n_ask2']\n",
    "tmp_df = train[train['file']=='snapshot_sym1_date0_am.csv'].reset_index(drop=True)[-2000:]\n",
    "tmp_df = tmp_df.reset_index(drop=True).reset_index()\n",
    "for num, col in enumerate(cols):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(4,1,num+1)\n",
    "    plt.plot(tmp_df['index'],tmp_df[col], color='red')\n",
    "    plt.title(col)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for num, col in enumerate(cols):\n",
    "    plt.plot(tmp_df['index'],tmp_df[col],label=col)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for num, col in enumerate(cols):\n",
    "    plt.plot(tmp_df['index'],tmp_df[col],label=col)\n",
    "\n",
    "plt.plot(tmp_df['index'],tmp_df['n_midprice'],label=\"n_midprice\",lw=3)\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['n_bid3','n_bid4','n_ask3','n_ask4']\n",
    "tmp_df = train[train['file']=='snapshot_sym1_date0_am.csv'].reset_index(drop=True)[-2000:]\n",
    "tmp_df = tmp_df.reset_index(drop=True).reset_index()\n",
    "for num, col in enumerate(cols):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(4,1,num+1)\n",
    "    plt.plot(tmp_df['index'],tmp_df[col], color='red')\n",
    "    plt.title(col)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for num, col in enumerate(cols):\n",
    "    plt.plot(tmp_df['index'],tmp_df[col],label=col)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for num, col in enumerate(cols):\n",
    "    plt.plot(tmp_df['index'],tmp_df[col],label=col)\n",
    "\n",
    "plt.plot(tmp_df['index'],tmp_df['n_midprice'],label=\"n_midprice\",lw=3)\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['n_bid5','n_bid4','n_ask5','n_ask4']\n",
    "tmp_df = train[train['file']=='snapshot_sym1_date0_am.csv'].reset_index(drop=True)[-2000:]\n",
    "tmp_df = tmp_df.reset_index(drop=True).reset_index()\n",
    "for num, col in enumerate(cols):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(4,1,num+1)\n",
    "    plt.plot(tmp_df['index'],tmp_df[col], color='red')\n",
    "    plt.title(col)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for num, col in enumerate(cols):\n",
    "    plt.plot(tmp_df['index'],tmp_df[col],label=col)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "for num, col in enumerate(cols):\n",
    "    plt.plot(tmp_df['index'],tmp_df[col],label=col)\n",
    "\n",
    "plt.plot(tmp_df['index'],tmp_df['n_midprice'],label=\"n_midprice\",lw=3)\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['amount_delta']\n",
    "tmp_df = train[train['file']=='snapshot_sym1_date0_am.csv'].reset_index(drop=True)[1:500]\n",
    "tmp_df = tmp_df.reset_index(drop=True).reset_index()\n",
    "for num, col in enumerate(cols):\n",
    "    plt.figure(figsize=(15,50))\n",
    "    plt.subplot(4,1,num+1)\n",
    "    plt.plot(tmp_df['index'],tmp_df[col])\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 波动率是给定股票价格变化的重要统计指标，因此要计算价格变化，我们首先需要在固定间隔进行股票估值。\n",
    "# 我们将使用已提供的数据的加权平均价格（WAP）进行可视化，WAP的变化反映股票波动情况。\n",
    "train['wap1'] = (train['n_bid1']*train['n_bsize1'] + train['n_ask1']*train['n_asize1'])/(train['n_bsize1'] + train['n_asize1'])\n",
    "test['wap1'] = (test['n_bid1']*test['n_bsize1'] + test['n_ask1']*test['n_asize1'])/(test['n_bsize1'] + test['n_asize1'])\n",
    "\n",
    "tmp_df = train[train['file']=='snapshot_sym1_date0_am.csv'].reset_index(drop=True)[-2000:]\n",
    "tmp_df = tmp_df.reset_index(drop=True).reset_index()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(tmp_df['index'], tmp_df['wap1'], color='red')\n",
    "plt.ylabel('Price Volatility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间相关特征\n",
    "train['hour'] = train['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "test['hour'] = test['time'].apply(lambda x:int(x.split(':')[0]))\n",
    "\n",
    "train['minute'] = train['time'].apply(lambda x:int(x.split(':')[1]))\n",
    "test['minute'] = test['time'].apply(lambda x:int(x.split(':')[1]))\n",
    "\n",
    "# 为了保证时间顺序的一致性，故进行排序\n",
    "train = train.sort_values(['file','time'])\n",
    "test = test.sort_values(['file','time'])\n",
    "\n",
    "# 当前时间特征\n",
    "# 构建买一卖一和买二卖二相关特征\n",
    "train['wap1'] = (train['n_bid1']*train['n_bsize1'] + train['n_ask1']*train['n_asize1'])/(train['n_bsize1'] + train['n_asize1'])\n",
    "test['wap1'] = (test['n_bid1']*test['n_bsize1'] + test['n_ask1']*test['n_asize1'])/(test['n_bsize1'] + test['n_asize1'])\n",
    "\n",
    "train['wap2'] = (train['n_bid2']*train['n_bsize2'] + train['n_ask2']*train['n_asize2'])/(train['n_bsize2'] + train['n_asize2'])\n",
    "test['wap2'] = (test['n_bid2']*test['n_bsize2'] + test['n_ask2']*test['n_asize2'])/(test['n_bsize2'] + test['n_asize2'])\n",
    "\n",
    "train['wap_balance'] = abs(train['wap1'] - train['wap2'])\n",
    "train['price_spread'] = (train['n_ask1'] - train['n_bid1']) / ((train['n_ask1'] + train['n_bid1'])/2)\n",
    "train['bid_spread'] = train['n_bid1'] - train['n_bid2']\n",
    "train['ask_spread'] = train['n_ask1'] - train['n_ask2']\n",
    "train['total_volume'] = (train['n_asize1'] + train['n_asize2']) + (train['n_bsize1'] + train['n_bsize2'])\n",
    "train['volume_imbalance'] = abs((train['n_asize1'] + train['n_asize2']) - (train['n_bsize1'] + train['n_bsize2']))\n",
    "\n",
    "test['wap_balance'] = abs(test['wap1'] - test['wap2'])\n",
    "test['price_spread'] = (test['n_ask1'] - test['n_bid1']) / ((test['n_ask1'] + test['n_bid1'])/2)\n",
    "test['bid_spread'] = test['n_bid1'] - test['n_bid2']\n",
    "test['ask_spread'] = test['n_ask1'] - test['n_ask2']\n",
    "test['total_volume'] = (test['n_asize1'] + test['n_asize2']) + (test['n_bsize1'] + test['n_bsize2'])\n",
    "test['volume_imbalance'] = abs((test['n_asize1'] + test['n_asize2']) - (test['n_bsize1'] + test['n_bsize2']))\n",
    "\n",
    "# 历史平移\n",
    "# 获取历史信息\n",
    "for val in ['wap1','wap2','wap_balance','price_spread','bid_spread','ask_spread','total_volume','volume_imbalance']:\n",
    "    for loc in [1,5,10,20,40,60]:\n",
    "        train[f'file_{val}_shift{loc}'] = train.groupby(['file'])[val].shift(loc)\n",
    "        test[f'file_{val}_shift{loc}'] = test.groupby(['file'])[val].shift(loc)\n",
    "    \n",
    "# 差分特征\n",
    "# 获取与历史数据的增长关系\n",
    "for val in ['wap1','wap2','wap_balance','price_spread','bid_spread','ask_spread','total_volume','volume_imbalance']:\n",
    "    for loc in [1,5,10,20,40,60]:\n",
    "        train[f'file_{val}_diff{loc}'] = train.groupby(['file'])[val].diff(loc)\n",
    "        test[f'file_{val}_diff{loc}'] = test.groupby(['file'])[val].diff(loc)\n",
    "    \n",
    "# 窗口统计\n",
    "# 获取历史信息分布变化信息\n",
    "# 可以尝试更多窗口大小已经统计方式，如min、max、median等\n",
    "for val in ['wap1','wap2','wap_balance','price_spread','bid_spread','ask_spread','total_volume','volume_imbalance']:\n",
    "    train[f'file_{val}_win7_mean'] = train.groupby(['file'])[val].transform(lambda x: x.rolling(window=7, min_periods=3).mean())\n",
    "    train[f'file_{val}_win7_std'] = train.groupby(['file'])[val].transform(lambda x: x.rolling(window=7, min_periods=3).std())\n",
    "    \n",
    "    test[f'file_{val}_win7_mean'] = test.groupby(['file'])[val].transform(lambda x: x.rolling(window=7, min_periods=3).mean())\n",
    "    test[f'file_{val}_win7_std'] = test.groupby(['file'])[val].transform(lambda x: x.rolling(window=7, min_periods=3).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_model(clf, train_x, train_y, test_x, test_y, clf_name, seed = 2023):\n",
    "    folds = 5\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    test_predict = np.zeros([test_x.shape[0], 3])\n",
    "    \n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('{}* is training'.format(str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]\n",
    "\n",
    "        if clf_name == \"xgb\":\n",
    "            # xgboost\n",
    "            xgb_params = {\n",
    "              'booster': 'gbtree', \n",
    "              'objective': 'multi:softprob',\n",
    "              'num_class':3,\n",
    "              'max_depth': 5,\n",
    "              'lambda': 10,\n",
    "              'subsample': 0.7,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'colsample_bylevel': 0.7,\n",
    "              'eta': 0.1,\n",
    "              'tree_method': 'hist',\n",
    "              'seed': 2023,\n",
    "              'nthread': 16,\n",
    "              }\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            \n",
    "            model = clf.train(xgb_params, train_matrix, num_boost_round=200, evals=watchlist)\n",
    "            val_pred  = model.predict(valid_matrix)\n",
    "            test_pred = model.predict(test_matrix)\n",
    "            \n",
    "        if clf_name == \"c45\":\n",
    "            # c45决策树\n",
    "            model = clf(criterion='entropy',splitter='best',max_depth=5)\n",
    "            model.fit(trn_x, trn_y)\n",
    "            \n",
    "            val_pred  = model.predict_proba(val_x)\n",
    "            test_pred = model.predict_proba(test_x)\n",
    "        \n",
    "        if clf_name == \"cart\":\n",
    "            # cart决策树\n",
    "            model = clf(criterion='gini',splitter='best',max_depth=5)\n",
    "            model.fit(trn_x, trn_y)\n",
    "            \n",
    "            val_pred  = model.predict_proba(val_x)\n",
    "            test_pred = model.predict_proba(test_x)\n",
    "\n",
    "        test_predict += test_pred / kf.n_splits\n",
    "\n",
    "    test_label = np.argmax(test_predict, axis=1)\n",
    "    F1_score = f1_score(test_y, test_label, average='micro')\n",
    "    print('F1_score:',F1_score)\n",
    "    return F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理train_x和test_x中的NaN值\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "# 处理train_x和test_x中的Inf值\n",
    "train = train.replace([np.inf, -np.inf], 0)\n",
    "test = test.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# 把作预测的数据集的特征提取出来\n",
    "cols_x = [f for f in test.columns if f not in \n",
    "          ['uuid','time','file','label_5','label_10','label_20','label_40','label_60']]\n",
    "\n",
    "# 入模特征\n",
    "cols = [f for f in test.columns if f not in ['uuid','time','file']]\n",
    "for label in ['label_5','label_10','label_20','label_40','label_60']:\n",
    "# for label in ['label_5']:\n",
    "    print(f'==== {label} ====')\n",
    "    # 选择c4.5模型\n",
    "    c45_test = cv_model(tree.DecisionTreeClassifier,\n",
    "                        train[cols_x], train[label], test[cols_x], test[label], 'c45')\n",
    "    # 选择cart模型\n",
    "    cart_test = cv_model(tree.DecisionTreeClassifier,\n",
    "                         train[cols_x], train[label], test[cols_x], test[label], 'cart')\n",
    "    # 选择xgboost模型\n",
    "    xgb_test = cv_model(xgb, train[cols_x], train[label], test[cols_x], test[label], 'xgb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
